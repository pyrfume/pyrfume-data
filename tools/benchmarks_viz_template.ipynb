{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c79f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrfume\n",
    "import pandas as pd\n",
    "import pyrfume.benchmarking as pbm\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8958b3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "archive = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2cc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data, models (unfit), and data-prep functions\n",
    "results = pbm.gridsearch_csv_to_frame('benchmarking.csv')\n",
    "prepare_dataset, models = pbm.load_pickle()\n",
    "\n",
    "# Fit models\n",
    "models['fitted_model'] = models.apply(\n",
    "    lambda row: pbm.fit_model_for_pickle(\n",
    "        archive=archive,\n",
    "        prepare_dataset=prepare_dataset,\n",
    "        row=row\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "models.set_index(['target', 'features', 'metric'], inplace=True)\n",
    "models['estimator'] = models['pipeline_steps'].astype(str).str[1:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879fde4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "best_results = pbm.get_best_results(results)\n",
    "best_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa462f92",
   "metadata": {},
   "source": [
    "# I. Landscape of scores across models and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233aa131",
   "metadata": {},
   "source": [
    "This section compares model scores in the overall parameter space explored, for all estimator families that were part of the gridsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bcd68",
   "metadata": {},
   "source": [
    "## 1. Heat maps of scores\n",
    "This section iterates through each combination of targets and features, and shows best scores for all model families (rows) vs. metrics (columns). The single best performing (model, metric) pair is indicated by a blue box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c31e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat maps\n",
    "targets = best_results.index.get_level_values(level='target').unique()\n",
    "features = best_results.index.get_level_values(level='features').unique()\n",
    "\n",
    "for target, feature in itertools.product(targets, features):\n",
    "    plt.title(f'Score summary: feature set={feature}; targets={target}')\n",
    "    pbm.plot_heatmap(best_results.xs((target, feature), level=['target','features']), show_rect=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606e9a4",
   "metadata": {},
   "source": [
    "## 2. Score report\n",
    "This section iterates through each target, and shows a strip-chart with the leading 10 scores for all estimator classes for that target (rows). Scores for all metrics are shown (columns). Both mordred and morgan features are shown on the same axes, for easy comparison. These plots are useful for visualizing intra and inter-model variability, as well as for discerning whether one feature set leads to systematically stronger predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559ffec",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "valid_metrics = [colname for colname in results.columns if colname.startswith('mean_')]\n",
    "targets = results.index.get_level_values(level='target').unique()\n",
    "\n",
    "for target in targets:\n",
    "    results_by_target = results.xs(target, level='target', drop_level=False)\n",
    "    pbm.plot_score_report(results_by_target, features.to_list(), valid_metrics, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d25a4c",
   "metadata": {},
   "source": [
    "## 3. Score distributions\n",
    "A summary version of the score plot. Shows score distributions for all models (rows) vs. metrics (columns) as box and whisker plots. Allows for easy comparison of central tendency across models as well as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_names = [name.replace('mean_','') for name in valid_metrics]\n",
    "name_map = {k: v for k, v in zip(valid_metrics, short_names)}\n",
    "\n",
    "df = results[valid_metrics].reset_index()\n",
    "df = df.melt(id_vars='pipeline_string', value_vars=valid_metrics, value_name='score', var_name='metric')\n",
    "df['metric'] = df['metric'].map(name_map)\n",
    "sns.catplot(data=df, x='score', y='pipeline_string', col='metric', kind='box', height=10, aspect=.3, palette='tab10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f6370",
   "metadata": {},
   "source": [
    "# II. Evaluation of top performing models\n",
    "This section shows a more granular and detailed view of the performance of individual models. For each dataset target, it shows the best-peforming model for the best feature set, across all metrics. For classification tasks, ROC curves and confusion matrices are plotted. For regression tasks, fit residuals and actual-v-predicted plots are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbm.iterate_and_plot_models(models, prepare_dataset, archive)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
